# Теория вероятностей

[http://statistics.zone/](http://statistics.zone/)

​[https://github.com/mavam/stat-cookbook/releases/download/0.2.6/stat-cookbook.pdf](https://github.com/mavam/stat-cookbook/releases/download/0.2.6/stat-cookbook.pdf)

Вероятностное пространств состоит из множества всех возможных элементарных исходов, а любое подмножество этих исходов представляет собой случайное событие.

Вероятность случайного события E обозначается P\(E\)

Два события E и F называются **зависимыми**, если знание о наступлении события E, даёт нам какую-то информацию о наступлении события F и наоборот, в противном случае они **независимые**.

С точки зрения математики говорят, что два события E и F независимы, если вероятность их совместного наступления равна произведению вероятностей их наступления по отдельности:

$$
P (E, F) = P (E) * P (F)
$$

## Условная вероятность

Если события E и F зависимы и при этом вероятность события F не равна 0, то условная вероятность события E при условии события F определяется как:

$$
P (E | F) = P (E, F) / P (F)
$$

Попытка вывести формулу выше:

$$
P(E, F) * 1/P(F) = b/c * 1/a/c = b/c * c/a = b/a
$$



Пространство вариантов состоит из **c** исходов. **a** исходов - включают в себя событие P\(F\). **b** исходов - включают в себя одновременно и событие P\(E\) и событие P\(F\). **b** - это подмножество **a**. Если событие P\(F\) произошло, а произойти оно могло через **a** исходов, то значит, что вероятность наступления события P\(E\) в этом случае - это **b/a.**

Из формулы условной вероятности также выводится:

$$
P(E, F) = P(E|F)*P(F)
$$

## Теорема Байеса

Позволяет переставить условные вероятности местами. Общая формулировка: Пусть нужно узнать вероятность некоего события E, зависящего он наступления некоего другого события F, причём нам известна лишь информация о вероятности события F, зависящего от наступления события E.

$$
P(E|F) = P(E, F)/F(F) = P(F|E)P(E)/P(F)
$$

Если событие F разложить на два взаимоисключающих события  - событие "F и E" и событие "F и не E", тогда:

$$
P(F) = P(F, E) + P(F, !E)
$$

В результате формула приводится к итоговому виду:

$$
P(E|F) = \frac {P(F|E)P(E)}{P(F|E)P(E)+P(F|!E)P(!E)}
$$

## Случайные величины

Это переменные, чьим возможным значениям поставлено в соответствие распределение вероятностей. Связанное со случайной величиной распределение вероятностей предоставляет ей вероятности, с которыми она реализует каждое из своих возможных значений.

**Среднее ожидаемое значение или математическое ожидание случайной величины - это взвешенная сумма произведений каждого её значения на его вероятность.**

Математическое ожидание броска монеты 1/2 = 0\*1/2+1\*1/2. Математическое ожидание диапазона range\(10\) = 4.5

На практике при анализе выборок математическое ожидание, как правило, неизвестно. Поэтому вместо него используют его оценку - среднее арифметическое.

Случайне величины могут обуславливаться вероятностью событий.

## Непрерывное распределение

Бросание монеты - это дискретное распределение - оно ставит в соответствие положительную вероятность пронумерованным исходам дискретного вероятностного пространства. 

Однако нередко требуется моделировать распределения на непрерывном пространстве исходов.

Поскольку между 0 и 1 находится бесконечное количество чисел, то значит, вес, который оно назначает индивидуальным точкам, должен с неизбежностью быть равен 0. По этой причине непрерывное распределение расстояния вероятностей представляют плотностью распределения вероятностей \(probability density function, pdf\), также именуемой _дифференциальной функцией распределения \(ДФР\)_, такой, что вероятность наблюдать значение в определённом интервале равна интегралу от дифференциальной функции, взятому в этих пределах.

Кумулятивная функция распределения  \(cumulative dictribution function, cdf\) или Интегральная функция распределения  \(ИФР\) - определяет вероятность, что случайная величина меньше или равна некоторому значению.

## Нормальное распределение

Определяется двумя параметрами мю - математическим ожиданием \(средним значением\) и сигма - стандартным отклонением. Математическое ожидание указывает на смещение колокола, а стандартное отклонение - на его ширину или масштаб.

ДФР нормального распределения:

$$
f(x|\mu, \sigma) = \frac{1}{\sqrt{2\pi\sigma}}\exp^{(-\frac{{(x-\mu)}^2}{2\sigma^2})}
$$

При mu=0 и sigma=1 оно называется стандартным нормальным распределением.

Если Z - это стандартная нормально распределённая случайная величина, то оказывается, что

$$
X=\sigma Z+u
$$

тоже является нормально распределённой, но с математическим ожиданием mu и стандартным отклонением sigma. И наоборот, если X - нормально распределённая случайная величина с математическим ожиданием mu и стандартным отклонением sigma, то

$$
Z=\frac{X-\mu}{\sigma}
$$

есть стандартная нормально распределённая случайная величина.

### ИФР нормального распределения

## Центральная предельная теорема

По существу: случайная величина, определённая как среднее большого числа независимых и идентично распределённых случайных величин, сама является приближённо нормально распределённой.

В частности, если x\_1, .. x\_n - случайные величины с математическим ожиданием mu и стандартным отклонением sigma и если n большое, то

$$
\frac{1}{n}(x_1+...+x_n)
$$

приближённо нормально распределённая величина с математическим ожиданием mu и стандартным отклонением, равным sigma/sqrt\(n\). Эквивалентным образом и чаще с большей практической пользой

$$
\frac{(x_1+...+x_n)-\mu n}{\sigma \sqrt{n}}
$$

есть приближённо нормально распределённая величина с нулевым математическим ожиданием и стандартным отклонением, равным 1.

